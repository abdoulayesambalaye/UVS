{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet Final Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nom Etudiant :**  SYLL\n",
    "\n",
    "**Prenom Etudiant:**  ABDOULAYE\n",
    "\n",
    "**Classe :**  MASTER 1 BIG DATA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "Ce projet consiste à utiliser Apache Spark pour faire l'analyse et le traitement des données de **[San Francisco Fire Department Calls ](https://data.sfgov.org/Public-Safety/Fire-Department-Calls-for-Service/nuek-vuh3)** afin de fournir quelques KPI (*Key Performance Indicator*). Le **SF Fire Dataset** comprend les réponses aux appels de toutes les unités d'incendie. Chaque enregistrement comprend le numéro d'appel, le numéro d'incident, l'adresse, l'identifiant de l'unité, le type d'appel et la disposition. Tous les intervalles de temps pertinents sont également inclus. Étant donné que ce Dataset est basé sur les réponses et que la plupart des appels impliquent plusieurs unités, ainsi il existe plusieurs enregistrements pour chaque numéro d'appel. Les adresses sont associées à un numéro de bloc, à une intersection ou à une boîte d'appel, et non à une adresse spécifique.\n",
    "\n",
    "**Plus de details sur la description des données cliquer sur ce [lien](https://data.sfgov.org/Public-Safety/Fire-Department-Calls-for-Service/nuek-vuh3)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Travail à faire.\n",
    "L'objectif de ce projet est de comprendre le **SF Fire Dataset** afin de bien répondre aux questions en utilisant les codes Spark/Scala adéquates.\n",
    "\n",
    "- Créer un repos git (public ou privé) et partager le repos avec mon mail (limahin10@gmail.com)\n",
    "- Ecrire un code lisible et bien indenté \n",
    "- N'oublier pas de mettre en commentaire la justification de vos réponses sur les cellules Markdown. \n",
    "\n",
    "\n",
    "## Note:\n",
    "- Le projet est personnel, c'est-à-dire chaque notebook ne concerne qu'un seul étudiant. \n",
    "- Deadline : **Jeudi 10 janvier 2021** (Aucune de dérogation ne sera acceptée)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation des packages Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Int = 2\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.functions._\n",
       "import spark.implicits._\n",
       "import org.apache.spark.sql.{functions=>F}\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._ \n",
    "import org.apache.spark.sql.functions._ \n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql.{functions => F}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons jeter un coup d'oeil sur la structure des données avant de définir un schéma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CallNumber,UnitID,IncidentNumber,CallType,CallDate,WatchDate,CallFinalDisposition,AvailableDtTm,Address,City,Zipcode,Battalion,StationArea,Box,OriginalPriority,Priority,FinalPriority,ALSUnit,CallTypeGroup,NumAlarms,UnitType,UnitSequenceInCallDispatch,FirePreventionDistrict,SupervisorDistrict,Neighborhood,Location,RowID,Delay\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!head -1 \"datasets/sf-fire/sf-fire-calls.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vu que la taille de ces données est énormes, inferer le schema pour un très grande volumes de données s'avère un peu couteux. Nous allons ainsi définir un schema pour le Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fireSchema: org.apache.spark.sql.types.StructType = StructType(StructField(CallNumber,IntegerType,true), StructField(UnitID,StringType,true), StructField(IncidentNumber,IntegerType,true), StructField(CallType,StringType,true), StructField(CallDate,StringType,true), StructField(WatchDate,StringType,true), StructField(CallFinalDisposition,StringType,true), StructField(AvailableDtTm,StringType,true), StructField(Address,StringType,true), StructField(City,StringType,true), StructField(Zipcode,IntegerType,true), StructField(Battalion,StringType,true), StructField(StationArea,StringType,true), StructField(Box,StringType,true), StructField(OriginalPriority,StringType,true), StructField(Priority,StringType,true), StructField(FinalPriority,IntegerType,true), StructField(ALSUnit,BooleanType,true)..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fireSchema = StructType(Array(StructField(\"CallNumber\", IntegerType, true),\n",
    "  StructField(\"UnitID\", StringType, true),\n",
    "  StructField(\"IncidentNumber\", IntegerType, true),\n",
    "  StructField(\"CallType\", StringType, true),                  \n",
    "  StructField(\"CallDate\", StringType, true),      \n",
    "  StructField(\"WatchDate\", StringType, true),\n",
    "  StructField(\"CallFinalDisposition\", StringType, true),\n",
    "  StructField(\"AvailableDtTm\", StringType, true),\n",
    "  StructField(\"Address\", StringType, true),       \n",
    "  StructField(\"City\", StringType, true),       \n",
    "  StructField(\"Zipcode\", IntegerType, true),       \n",
    "  StructField(\"Battalion\", StringType, true),                 \n",
    "  StructField(\"StationArea\", StringType, true),       \n",
    "  StructField(\"Box\", StringType, true),       \n",
    "  StructField(\"OriginalPriority\", StringType, true),       \n",
    "  StructField(\"Priority\", StringType, true),       \n",
    "  StructField(\"FinalPriority\", IntegerType, true),       \n",
    "  StructField(\"ALSUnit\", BooleanType, true),       \n",
    "  StructField(\"CallTypeGroup\", StringType, true),\n",
    "  StructField(\"NumAlarms\", IntegerType, true),\n",
    "  StructField(\"UnitType\", StringType, true),\n",
    "  StructField(\"UnitSequenceInCallDispatch\", IntegerType, true),\n",
    "  StructField(\"FirePreventionDistrict\", StringType, true),\n",
    "  StructField(\"SupervisorDistrict\", StringType, true),\n",
    "  StructField(\"Neighborhood\", StringType, true),\n",
    "  StructField(\"Location\", StringType, true),\n",
    "  StructField(\"RowID\", StringType, true),\n",
    "  StructField(\"Delay\", FloatType, true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sfFireFile: String = datasets/sf-fire/sf-fire-calls.csv\n",
       "fireDF: org.apache.spark.sql.DataFrame = [CallNumber: int, UnitID: string ... 26 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sfFireFile = \"datasets/sf-fire/sf-fire-calls.csv\"\n",
    "val fireDF = spark\n",
    "  .read\n",
    "  .schema(fireSchema)\n",
    "  .option(\"header\", \"true\")\n",
    "  .csv(sfFireFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons mettre en cache le Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: fireDF.type = [CallNumber: int, UnitID: string ... 26 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fireDF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "java.lang.IllegalArgumentException",
     "evalue": " Unsupported class file major version 55",
     "output_type": "error",
     "traceback": [
      "java.lang.IllegalArgumentException: Unsupported class file major version 55",
      "  at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:166)",
      "  at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:148)",
      "  at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:136)",
      "  at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:237)",
      "  at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:50)",
      "  at org.apache.spark.util.FieldAccessFinder$$anon$4$$anonfun$visitMethodInsn$7.apply(ClosureCleaner.scala:845)",
      "  at org.apache.spark.util.FieldAccessFinder$$anon$4$$anonfun$visitMethodInsn$7.apply(ClosureCleaner.scala:828)",
      "  at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)",
      "  at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)",
      "  at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)",
      "  at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)",
      "  at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)",
      "  at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:134)",
      "  at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)",
      "  at org.apache.spark.util.FieldAccessFinder$$anon$4.visitMethodInsn(ClosureCleaner.scala:828)",
      "  at org.apache.xbean.asm6.ClassReader.readCode(ClassReader.java:2175)",
      "  at org.apache.xbean.asm6.ClassReader.readMethod(ClassReader.java:1238)",
      "  at org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:631)",
      "  at org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:355)",
      "  at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:272)",
      "  at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:271)",
      "  at scala.collection.immutable.List.foreach(List.scala:392)",
      "  at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:271)",
      "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:163)",
      "  at org.apache.spark.SparkContext.clean(SparkContext.scala:2326)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2100)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)",
      "  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)",
      "  at org.apache.spark.rdd.RDD.collect(RDD.scala:989)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)",
      "  at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2836)",
      "  at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2835)",
      "  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)",
      "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)",
      "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3369)",
      "  at org.apache.spark.sql.Dataset.count(Dataset.scala:2835)",
      "  ... 43 elided",
      ""
     ]
    }
   ],
   "source": [
    "fireDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CallNumber: integer (nullable = true)\n",
      " |-- UnitID: string (nullable = true)\n",
      " |-- IncidentNumber: integer (nullable = true)\n",
      " |-- CallType: string (nullable = true)\n",
      " |-- CallDate: string (nullable = true)\n",
      " |-- WatchDate: string (nullable = true)\n",
      " |-- CallFinalDisposition: string (nullable = true)\n",
      " |-- AvailableDtTm: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- Battalion: string (nullable = true)\n",
      " |-- StationArea: string (nullable = true)\n",
      " |-- Box: string (nullable = true)\n",
      " |-- OriginalPriority: string (nullable = true)\n",
      " |-- Priority: string (nullable = true)\n",
      " |-- FinalPriority: integer (nullable = true)\n",
      " |-- ALSUnit: boolean (nullable = true)\n",
      " |-- CallTypeGroup: string (nullable = true)\n",
      " |-- NumAlarms: integer (nullable = true)\n",
      " |-- UnitType: string (nullable = true)\n",
      " |-- UnitSequenceInCallDispatch: integer (nullable = true)\n",
      " |-- FirePreventionDistrict: string (nullable = true)\n",
      " |-- SupervisorDistrict: string (nullable = true)\n",
      " |-- Neighborhood: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- RowID: string (nullable = true)\n",
      " |-- Delay: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fireDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------+--------------------------+----------------------+------------------+--------------------+--------------------+-------------+---------+\n",
      "|CallNumber|UnitID|IncidentNumber|        CallType|  CallDate| WatchDate|CallFinalDisposition|       AvailableDtTm|             Address|City|Zipcode|Battalion|StationArea| Box|OriginalPriority|Priority|FinalPriority|ALSUnit|CallTypeGroup|NumAlarms|UnitType|UnitSequenceInCallDispatch|FirePreventionDistrict|SupervisorDistrict|        Neighborhood|            Location|        RowID|    Delay|\n",
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------+--------------------------+----------------------+------------------+--------------------+--------------------+-------------+---------+\n",
      "|  20110016|   T13|       2003235|  Structure Fire|01/11/2002|01/10/2002|               Other|01/11/2002 01:51:...|2000 Block of CAL...|  SF|  94109|      B04|         38|3362|               3|       3|            3|  false|         null|        1|   TRUCK|                         2|                     4|                 5|     Pacific Heights|(37.7895840679362...|020110016-T13|     2.95|\n",
      "|  20110022|   M17|       2003241|Medical Incident|01/11/2002|01/10/2002|               Other|01/11/2002 03:01:...|0 Block of SILVER...|  SF|  94124|      B10|         42|6495|               3|       3|            3|   true|         null|        1|   MEDIC|                         1|                    10|                10|Bayview Hunters P...|(37.7337623673897...|020110022-M17|      4.7|\n",
      "|  20110023|   M41|       2003242|Medical Incident|01/11/2002|01/10/2002|               Other|01/11/2002 02:39:...|MARKET ST/MCALLIS...|  SF|  94102|      B03|         01|1455|               3|       3|            3|   true|         null|        1|   MEDIC|                         2|                     3|                 6|          Tenderloin|(37.7811772186856...|020110023-M41|2.4333334|\n",
      "|  20110032|   E11|       2003250|    Vehicle Fire|01/11/2002|01/10/2002|               Other|01/11/2002 04:16:...|APPLETON AV/MISSI...|  SF|  94110|      B06|         32|5626|               3|       3|            3|  false|         null|        1|  ENGINE|                         1|                     6|                 9|      Bernal Heights|(37.7388432849018...|020110032-E11|      1.5|\n",
      "|  20110043|   B04|       2003259|          Alarms|01/11/2002|01/10/2002|               Other|01/11/2002 06:01:...|1400 Block of SUT...|  SF|  94109|      B04|         03|3223|               3|       3|            3|  false|         null|        1|   CHIEF|                         2|                     4|                 2|    Western Addition|(37.7872890372638...|020110043-B04|3.4833333|\n",
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------+--------------------------+----------------------+------------------+--------------------+--------------------+-------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fireDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtrage des d'appels de type \"Medical Incident\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------+--------------+\n",
      "|IncidentNumber|AvailableDtTm         |CallType      |\n",
      "+--------------+----------------------+--------------+\n",
      "|2003235       |01/11/2002 01:51:44 AM|Structure Fire|\n",
      "|2003250       |01/11/2002 04:16:46 AM|Vehicle Fire  |\n",
      "|2003259       |01/11/2002 06:01:58 AM|Alarms        |\n",
      "|2003279       |01/11/2002 08:03:26 AM|Structure Fire|\n",
      "|2003301       |01/11/2002 09:46:44 AM|Alarms        |\n",
      "+--------------+----------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fewFireDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [IncidentNumber: int, AvailableDtTm: string ... 1 more field]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fewFireDF = fireDF\n",
    "  .select(\"IncidentNumber\", \"AvailableDtTm\", \"CallType\") \n",
    "  .where($\"CallType\" =!= \"Medical Incident\")\n",
    "\n",
    "fewFireDF.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "**Combien de types d'appels distincts ont été passés ?**  \n",
    "Pour être sûr, il ne faut pas compter les valeurs «nulles» dans la colonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|   28|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// dans cette question j'ai pas compter les valeurs null c'est pourquoi j'ai enlever les na et aussi \n",
    "//mettre l'aggregat l'ensemble des données entier\n",
    "fireDF.na.drop.agg(countDistinct(col(\"CallType\")).alias(\"count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quels types d'appels différents ont été passés au service d'incendie?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|        CallType|\n",
      "+----------------+\n",
      "|  Structure Fire|\n",
      "|Medical Incident|\n",
      "|Medical Incident|\n",
      "|    Vehicle Fire|\n",
      "|          Alarms|\n",
      "|  Structure Fire|\n",
      "|          Alarms|\n",
      "|          Alarms|\n",
      "|Medical Incident|\n",
      "|Medical Incident|\n",
      "|Medical Incident|\n",
      "|  Structure Fire|\n",
      "|Medical Incident|\n",
      "|Medical Incident|\n",
      "|  Structure Fire|\n",
      "|  Structure Fire|\n",
      "|  Structure Fire|\n",
      "|Medical Incident|\n",
      "|Medical Incident|\n",
      "|Medical Incident|\n",
      "+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// dans cette question j'ai selectionner CallType qui permet de sortir l'ensemble des types d'appels differents\n",
    "fireDF.select(\"CallType\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "**Trouver toutes les réponses ou les délais sont supérieurs à 5 minutes?**\n",
    "\n",
    "*Indication\n",
    "1. Renommer la colonne Delay -> ReponseDelayedinMins\n",
    "2. Retourner un nouveau DataFrame\n",
    "3. Afficher tous les appels où le temps de réponse à un site d'incendie a eu lieu après un retard de plus de 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|ResponseDelayedinMins|\n",
      "+---------------------+\n",
      "|                 5.35|\n",
      "|                 6.25|\n",
      "|                  5.2|\n",
      "|                  5.6|\n",
      "|                 7.25|\n",
      "|            11.916667|\n",
      "|             5.116667|\n",
      "|             8.633333|\n",
      "|             95.28333|\n",
      "|                 5.45|\n",
      "|                  7.6|\n",
      "|             6.133333|\n",
      "|            5.1833334|\n",
      "|            6.9166665|\n",
      "|                  5.2|\n",
      "|                 6.35|\n",
      "|             7.983333|\n",
      "|                13.55|\n",
      "|                 5.15|\n",
      "|            13.583333|\n",
      "+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "newFireDF: org.apache.spark.sql.DataFrame = [CallNumber: int, UnitID: string ... 26 more fields]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// dans cette question après avoir renommé, j'ai fait un select sur les temps de réponses \n",
    "//et utiliser un where pour un delais supérieur à 5mn\n",
    "val newFireDF = fireDF.withColumnRenamed(\"Delay\", \"ResponseDelayedinMins\")\n",
    "newFireDF\n",
    ".select(\"ResponseDelayedinMins\")\n",
    ".where($\"ResponseDelayedinMins\">5)\n",
    ".show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations des dates\n",
    "Maintenant nous allons d'abord:\n",
    "1. Transformer les dates de type String en Spark Timestamp afin que nous puissions effectuer des requêtes basées sur la date plus tard\n",
    "2. Retourner le Dataframe transformée\n",
    "3. Mettre en cache le nouveau DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+\n",
      "|       IncidentDate|        OnWatchDate|      AvailableDtTS|\n",
      "+-------------------+-------------------+-------------------+\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 01:51:44|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 03:01:18|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 02:39:50|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 04:16:46|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 06:01:58|\n",
      "|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 08:03:26|\n",
      "|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 09:46:44|\n",
      "|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 09:58:53|\n",
      "|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 12:06:57|\n",
      "|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 13:08:40|\n",
      "|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 15:31:02|\n",
      "|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 14:59:04|\n",
      "|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 16:22:49|\n",
      "|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 16:18:33|\n",
      "|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 16:09:08|\n",
      "|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 16:09:08|\n",
      "|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 16:09:08|\n",
      "|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 16:34:23|\n",
      "|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 16:51:31|\n",
      "|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 16:51:12|\n",
      "+-------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fireTSDF: org.apache.spark.sql.DataFrame = [CallNumber: int, UnitID: string ... 26 more fields]\n",
       "res13: fireTSDF.type = [CallNumber: int, UnitID: string ... 26 more fields]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//dans cette question nous avons un nouveau dataframe et j'ai utilisé un select sur 3 colonnes pour convertir \n",
    "//les dates\n",
    "val fireTSDF = newFireDF\n",
    "  .withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\")).drop(\"CallDate\") \n",
    "  .withColumn(\"OnWatchDate\", to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\")).drop(\"WatchDate\") \n",
    "  .withColumn(\"AvailableDtTS\", to_timestamp(col(\"AvailableDtTm\"), \"MM/dd/yyyy hh:mm:ss a\")).drop(\"AvailableDtTm\")\n",
    "fireTSDF\n",
    ".select(\"IncidentDate\", \"OnWatchDate\", \"AvailableDtTS\")\n",
    ".show()\n",
    "\n",
    "fireTSDF.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "**Quels sont les types d'appels les plus courants?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            CallType| count|\n",
      "+--------------------+------+\n",
      "|    Medical Incident|113794|\n",
      "|      Structure Fire| 23319|\n",
      "|              Alarms| 19406|\n",
      "|   Traffic Collision|  7013|\n",
      "|Citizen Assist / ...|  2524|\n",
      "|               Other|  2166|\n",
      "|        Outside Fire|  2094|\n",
      "|        Vehicle Fire|   854|\n",
      "|Gas Leak (Natural...|   764|\n",
      "|        Water Rescue|   755|\n",
      "|Odor (Strange / U...|   490|\n",
      "|   Electrical Hazard|   482|\n",
      "|Elevator / Escala...|   453|\n",
      "|Smoke Investigati...|   391|\n",
      "|          Fuel Spill|   193|\n",
      "|              HazMat|   124|\n",
      "|Industrial Accidents|    94|\n",
      "|           Explosion|    89|\n",
      "|Train / Rail Inci...|    57|\n",
      "|  Aircraft Emergency|    36|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//dans cette question j'ai fait un select sur les types appels et enlever les valeurs nulls puis faire \n",
    "//un regroupement par type d'appel en les comptants grace à compte et les agencer par ordre decroissant\n",
    "fireTSDF\n",
    "    .select(\"CallType\")\n",
    "    .where(col(\"CallType\").isNotNull)\n",
    "    .groupBy(\"CallType\")\n",
    "    .count()\n",
    "    .orderBy(desc(\"count\"))\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5-a\n",
    "**Quels sont boites postaux rencontrés dans les appels les plus courants?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+-----+\n",
      "|Zipcode|        CallType|count|\n",
      "+-------+----------------+-----+\n",
      "|  94102|Medical Incident|16130|\n",
      "|  94103|Medical Incident|14775|\n",
      "|  94110|Medical Incident| 9995|\n",
      "|  94109|Medical Incident| 9479|\n",
      "|  94124|Medical Incident| 5885|\n",
      "|  94112|Medical Incident| 5630|\n",
      "|  94115|Medical Incident| 4785|\n",
      "|  94122|Medical Incident| 4323|\n",
      "|  94107|Medical Incident| 4284|\n",
      "|  94133|Medical Incident| 3977|\n",
      "|  94117|Medical Incident| 3522|\n",
      "|  94134|Medical Incident| 3437|\n",
      "|  94114|Medical Incident| 3225|\n",
      "|  94118|Medical Incident| 3104|\n",
      "|  94121|Medical Incident| 2953|\n",
      "|  94116|Medical Incident| 2738|\n",
      "|  94132|Medical Incident| 2594|\n",
      "|  94110|  Structure Fire| 2267|\n",
      "|  94105|Medical Incident| 2258|\n",
      "|  94102|  Structure Fire| 2229|\n",
      "+-------+----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//dans cette question j'ai fait un select sur les types d'appels et les boites postaux\n",
    "// et j'ai aussi utiliser un where pour enlever les boites postaux non null\n",
    "// et les regrouper par types d'appel et les compters par ordre décroissant\n",
    "fireTSDF\n",
    "    .select(\"CallType\", \"ZipCode\")\n",
    "    .where(col(\"Zipcode\").isNotNull)\n",
    "    .groupBy(\"Zipcode\", \"CallType\")\n",
    "    .count()\n",
    "    .orderBy(desc(\"count\"))\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5-a\n",
    "**Quels sont les quartiers de San Francisco dont les codes postaux sont 94102 et 94103?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+\n",
      "|Zipcode|    neighborhood|\n",
      "+-------+----------------+\n",
      "|  94103| South of Market|\n",
      "|  94103|         Mission|\n",
      "|  94103| South of Market|\n",
      "|  94103| South of Market|\n",
      "|  94103| South of Market|\n",
      "|  94102|      Tenderloin|\n",
      "|  94103| South of Market|\n",
      "|  94103| South of Market|\n",
      "|  94103| South of Market|\n",
      "|  94103| South of Market|\n",
      "|  94103|         Mission|\n",
      "|  94103|         Mission|\n",
      "|  94102|      Tenderloin|\n",
      "|  94102|      Tenderloin|\n",
      "|  94103| South of Market|\n",
      "|  94102|      Tenderloin|\n",
      "|  94102|      Tenderloin|\n",
      "|  94103| South of Market|\n",
      "|  94102|Western Addition|\n",
      "|  94103| South of Market|\n",
      "+-------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//dans cette question j'ai fait un select sur les boites postaux et les quartiers de san francisco(neighborhood)\n",
    "//j'ai utilisé un where pour selectionner seulement les quartier de san francisco\n",
    "//et j'ai utiliser un filter pour recupérer les boites postaux demandés\n",
    "\n",
    "\n",
    "fireTSDF\n",
    "    .select( \"Zipcode\", \"neighborhood\")\n",
    "    .where($\"City\"===\"San Francisco\")\n",
    "    .filter($\"Zipcode\"===94102 or $\"Zipcode\"===94103)\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "**Determiner le nombre total d'appels, ainsi que la moyenne, le minimum et le maximum du temps de réponse des appels?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------------+--------------------------+--------------------------+\n",
      "|count(CallType)|avg(ResponseDelayedinMins)|min(ResponseDelayedinMins)|max(ResponseDelayedinMins)|\n",
      "+---------------+--------------------------+--------------------------+--------------------------+\n",
      "|         175296|         3.892364154521585|               0.016666668|                   1844.55|\n",
      "+---------------+--------------------------+--------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//dans cette question j'ai utiliser une fonctions puis un select qui me permet de compter les types d'appels\n",
    "//de calculer la moyenne, le minimum et le maximum\n",
    "fireTSDF\n",
    ".select(count(\"CallType\"), F.avg(\"ResponseDelayedinMins\"),\n",
    "F.min(\"ResponseDelayedinMins\"), F.max(\"ResponseDelayedinMins\"))\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7-a\n",
    "**Combien d'années distinctes trouve t-on dans ce Dataset?**  \n",
    "Dans ce dataset nous avons des données comprises entre 2000-2018. Vous pouvez utilisez la fonction Spark `year()` pour les dates en Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|year(IncidentDate)|\n",
      "+------------------+\n",
      "|              2000|\n",
      "|              2001|\n",
      "|              2002|\n",
      "|              2003|\n",
      "|              2004|\n",
      "|              2005|\n",
      "|              2006|\n",
      "|              2007|\n",
      "|              2008|\n",
      "|              2009|\n",
      "|              2010|\n",
      "|              2011|\n",
      "|              2012|\n",
      "|              2013|\n",
      "|              2014|\n",
      "|              2015|\n",
      "|              2016|\n",
      "|              2017|\n",
      "|              2018|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//dans cette question j'ai utilisé une fonction et j'ai fait un select sur la colonne incidentdate et distinct()\n",
    "//pour distinguer les date et les regrouper par ordre croissant\n",
    "fireTSDF\n",
    ".select(year($\"IncidentDate\"))\n",
    ".distinct()\n",
    ".orderBy(year($\"IncidentDate\"))\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7-b\n",
    "**Quelle semaine de l'année 2018 a eu le plus d'appels d'incendie?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "41: error: value like is not a member of org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]",
     "output_type": "error",
     "traceback": [
      "<console>:41: error: value like is not a member of org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]",
      "           .where($\"CallType\").like(\"%Fire%\")",
      "                               ^",
      ""
     ]
    }
   ],
   "source": [
    "//Reponse 7-b\n",
    "fireTSDF\n",
    "    .where(year(col(\"IncidentDate\")) === 2018)\n",
    "    .where($\"CallType\").like(\"%Fire%\")\n",
    "    .select(weekofyear(col(\"IncidentDate\")).as(\"week\"))\n",
    "    .groupBy(\"week\")\n",
    "    .count()\n",
    "    .orderBy(desc(\"count\"))\n",
    "    .limit(1)\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "**Quels sont les quartiers de San Francisco qui ont connu le pire temps de réponse en 2018?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------------+\n",
      "|        Neighborhood|max(ResponseDelayedinMins)|\n",
      "+--------------------+--------------------------+\n",
      "|           Chinatown|                 491.26666|\n",
      "|Financial Distric...|                 406.63333|\n",
      "|          Tenderloin|                 340.48334|\n",
      "|      Haight Ashbury|                 175.86667|\n",
      "|Bayview Hunters P...|                     155.8|\n",
      "|     Pacific Heights|                 129.01666|\n",
      "|        Potrero Hill|                     109.8|\n",
      "|        Inner Sunset|                 106.13333|\n",
      "|     South of Market|                  94.71667|\n",
      "|      Inner Richmond|                 90.433334|\n",
      "|           Excelsior|                  83.76667|\n",
      "| Castro/Upper Market|                  74.13333|\n",
      "|    Western Addition|                 67.916664|\n",
      "|            Nob Hill|                     67.45|\n",
      "|             Mission|                 54.666668|\n",
      "|    Presidio Heights|                 52.883335|\n",
      "|       Outer Mission|                 43.383335|\n",
      "|  West of Twin Peaks|                      43.2|\n",
      "|         North Beach|                 40.933334|\n",
      "|            Presidio|                 38.816666|\n",
      "+--------------------+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//dans cette question j'ai utiliser un where pour selectionner les qurtier de san francisco\n",
    "// une fontion qui me permet de recuperer la date \n",
    "//un select sur les quartiers et les delais de reponses et les regrouper par quartier\n",
    "//j'ai utiliser l'aggregat pour les maximum temps de réponses et les regrouper par ordre decroissant\n",
    "fireTSDF\n",
    "    .where($\"City\" === \"San Francisco\")\n",
    "    .where(year(col(\"IncidentDate\")) === 2018)\n",
    "    .select(\"Neighborhood\", \"ResponseDelayedinMins\")\n",
    "    .groupBy(\"Neighborhood\")\n",
    "    .agg(max(\"ResponseDelayedinMins\"))\n",
    "    .orderBy(max(\"ResponseDelayedinMins\").desc).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "**Comment stocker les données du Dataframe sous format de fichiers Parquet?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "//dans cette question j'ai  un write.parquet pour le stockage\n",
    "newFireDF.write.parquet(\"tableau.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "**Comment relire les données stockée en format Parquet?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "java.lang.IllegalArgumentException",
     "evalue": " Unsupported class file major version 55",
     "output_type": "error",
     "traceback": [
      "java.lang.IllegalArgumentException: Unsupported class file major version 55",
      "  at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:166)",
      "  at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:148)",
      "  at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:136)",
      "  at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:237)",
      "  at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:50)",
      "  at org.apache.spark.util.FieldAccessFinder$$anon$4$$anonfun$visitMethodInsn$7.apply(ClosureCleaner.scala:845)",
      "  at org.apache.spark.util.FieldAccessFinder$$anon$4$$anonfun$visitMethodInsn$7.apply(ClosureCleaner.scala:828)",
      "  at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)",
      "  at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)",
      "  at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)",
      "  at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)",
      "  at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)",
      "  at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:134)",
      "  at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)",
      "  at org.apache.spark.util.FieldAccessFinder$$anon$4.visitMethodInsn(ClosureCleaner.scala:828)",
      "  at org.apache.xbean.asm6.ClassReader.readCode(ClassReader.java:2175)",
      "  at org.apache.xbean.asm6.ClassReader.readMethod(ClassReader.java:1238)",
      "  at org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:631)",
      "  at org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:355)",
      "  at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:272)",
      "  at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:271)",
      "  at scala.collection.immutable.List.foreach(List.scala:392)",
      "  at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:271)",
      "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:163)",
      "  at org.apache.spark.SparkContext.clean(SparkContext.scala:2326)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2100)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)",
      "  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)",
      "  at org.apache.spark.rdd.RDD.collect(RDD.scala:989)",
      "  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:635)",
      "  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:241)",
      "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:194)",
      "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:194)",
      "  at scala.Option.orElse(Option.scala:289)",
      "  at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:193)",
      "  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:387)",
      "  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:242)",
      "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:230)",
      "  at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:664)",
      "  at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:648)",
      "  ... 42 elided",
      ""
     ]
    }
   ],
   "source": [
    "//ici j'ai utiliser un read pour pouvoir le lire\n",
    "val parquetFileDF = spark.read.parquet(\"tableau.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
